% This must be in the first 5 lines to tell arXiv to use pdfLaTeX, which is strongly recommended.
\pdfoutput=1
% In particular, the hyperref package requires pdfLaTeX in order to break URLs across lines.

\documentclass[11pt]{article}

% Remove the "guidelines" option to generate the final version.
% \usepackage[guidelines]{reproducibility} % show guidelines
\usepackage[]{reproducibility} % hide guidelines


% Standard package includes
\usepackage{times}
\usepackage{latexsym}

% For proper rendering and hyphenation of words containing Latin characters (including in bib files)
\usepackage[T1]{fontenc}
% For Vietnamese characters
% \usepackage[T5]{fontenc}
% See https://www.latex-project.org/help/documentation/encguide.pdf for other character sets

% This assumes your files are encoded as UTF8
\usepackage[utf8]{inputenc}

% This is not strictly necessary, and may be commented out,
% but it will improve the layout of the manuscript,
% and will typically save some space.
\usepackage{microtype}
\usepackage{graphicx}
\usepackage{hyperref}
\usepackage{amsmath}
\usepackage{mathtools}
\usepackage{multirow}
\usepackage{listings}
\usepackage{xcolor}
\usepackage{booktabs} % for tables


% THE pdfinfo Title AND Author ARE NOT NECESSARY, THEY ARE METADATA FOR THE FINAL PDF FILE
\hypersetup{pdfinfo={
Title={Reproducibility in Machine Learning Research},
Author={Davide Domini}
}}
%\setcounter{secnumdepth}{0}  
 \begin{document}
%
\title{Reproducibility in Machine Learning Research}
\author{Davide Domini \\
University of Bologna -- Department of Computer Science and Engineering\\
davide.domini@unibo.it
}
\maketitle


%\attention{DO NOT MODIFY THIS TEMPLATE - EXCEPT, OF FOR TITLE, SUBTITLE AND AUTHORS.\\ IN THE FINAL VERSION, IN THE \LaTeX\ SOURCE REMOVE THE \texttt{guidelines} OPTION FROM  \texttt{$\backslash$usepackage[guidelines]\{nlpreport\}}.}

\begin{abstract}
Reproducibility is a cornerstone of scientific integrity but remains a major challenge 
 in the field of machine learning (ML). 
%
The non-deterministic nature of training processes, opaque preprocessing steps, 
 and poorly documented environments often lead to irreproducible results. 
%
This report outlines the core reproducibility issues in ML workflows, 
 illustrates how they manifest in practice, and discusses tools 
 and practices that can help mitigate them. 
% 
While full reproducibility is difficult to guarantee, adopting rigorous standards 
 can substantially improve robustness and transparency in ML-based research.
\end{abstract}

\section{Introduction}
\label{sec:introduction}
Reproducibility in scientific research refers to the ability to re-run an experiment and 
 obtain the same results using the same data and methods. 
%
In machine learning, this concept is critical but elusive. 
%
Unlike traditional experiments, ML pipelines often include stochastic processes 
 (e.g., weight initialization, dropout, random sampling), complex software dependencies, 
 and undocumented preprocessing---all of which hinder reproducibility.

Machine learning systems are often built on multi-layered toolchains that combine 
 code, data, software environments, and hardware settings. 
% 
This intricate composition makes the reproduction of experiments particularly sensitive to 
 any change in configuration, even seemingly minor ones. 
% 
For instance, updates in a deep learning framework or changes in random seed initialization 
 can yield notably different results. 
% 
Furthermore, the fast-paced nature of ML development exacerbates this issue: 
 researchers and engineers rapidly adopt new tools and libraries, 
 sometimes at the expense of careful documentation or environment control.

Recent studies~\cite{DBLP:journals/ncs/X21i,DBLP:conf/qrs/Rivera-LandosKN21} have shown that even 
 minor changes in software libraries or data formats can lead to significant discrepancies in 
 model performance. 
%
As a result, reproducing published ML results is often non-trivial, 
 undermining both trust and progress in the field.

\section{Problem Description}
\label{sec:problem}
A typical ML workflow includes several stages: data preprocessing, model training, validation, and evaluation. Each of these stages introduces potential sources of irreproducibility.

Firstly, the inherent randomness in model training—stemming from operations such as random weight initialization or dropout—can result in slightly different models even when trained on the same dataset. Unless all sources of randomness are controlled by setting consistent seeds across libraries like NumPy, PyTorch, or TensorFlow, results may vary between runs.

Another critical issue lies in environment inconsistencies. Machine learning code often depends on a complex stack of software libraries and frameworks. Even minor version differences can lead to substantial changes in behavior or performance, making it difficult to reproduce results unless the original environment is precisely replicated.

Data leakage is also a common and often unnoticed pitfall. Mistakes such as including test data during training or improperly shuffling data can lead to artificially high performance metrics that cannot be replicated. These bugs are particularly dangerous because they often go undetected until a failed reproduction attempt occurs.

Finally, a widespread lack of documentation contributes significantly to irreproducibility. Details such as the method of feature scaling, label encoding, or even the exact data preprocessing sequence are frequently omitted in publications, leaving others unable to accurately replicate the study.


\section{Discussion}
\label{sec:system}
Improving the reproducibility of machine learning research requires a combination of cultural changes and technical strategies.

Controlling randomness is one of the first steps toward repeatability. Setting fixed seeds for all relevant random number generators can significantly reduce variation between runs. However, even this does not always guarantee identical results, especially when working with GPUs or parallel computation environments.

Creating consistent and portable environments is another critical aspect. By using tools like Docker or Conda, researchers can define their execution environment, including all library versions, operating system dependencies, and configurations. This ensures that collaborators or future researchers can replicate the setup exactly as it was during the original experiment.

Tracking experiments systematically can also improve reproducibility. Platforms such as MLflow and Weights \& Biases allow researchers to log every detail of their runs, including hyperparameters, datasets, metrics, and model artifacts. This facilitates comparison, debugging, and replication.

Versioning of data is equally important. Data Version Control (DVC) extends Git-like tracking to datasets and models, ensuring that specific versions of data used for training or evaluation are not lost or overwritten over time.

Moreover, the use of workflow management tools such as Snakemake or Kedro introduces structure to the experimentation process. These tools help formalize and automate complex pipelines, ensuring that each step is well defined and reproducible.

Finally, publishing well-documented codebases is essential. Making repositories publicly available on platforms like GitHub, along with clear README files and requirements, should be a standard practice for any scientific ML project.

Despite these available tools and practices, reproducibility is still not universally prioritized. Academic and industry incentives often reward novelty and results over transparency and robustness. Until there is a cultural shift that values reproducibility as a core scientific metric, these technical solutions will remain underutilized.


\section{Conclusion}
\label{sec:conclusion}
Reproducibility in machine learning is not an illusion, but it is certainly fragile. Without deliberate practices and tooling, ML research is prone to producing results that cannot be verified or trusted. Fortunately, the ecosystem now offers mature solutions for many of the reproducibility pitfalls, and their adoption is gradually increasing. By combining transparency, automation, and environment control, researchers can make significant strides toward more robust and trustworthy ML science.

% \bibliographystyle{unsrt}
\bibliography{biblio}

\end{document}